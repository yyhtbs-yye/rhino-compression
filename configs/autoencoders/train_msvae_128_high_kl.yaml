_import: configs/train_base_config.yaml

boat:
  path: 'rhcompression.boats.msvae_boat'
  name: 'MSVAEBoat'

  hyperparameters:
    beta_kl: 0.001
    weight_adv_patch: 0.5
    cut_freq: 0.33

  models:
    net:
      path: 'rhcompression.nn.model_zoo.autoencoder_kl'
      name: 'AutoencoderKL'
      params: 
        ddconfig:
          double_z: True
          z_channels: 16
          resolution: 128
          in_channels: 3
          out_ch: 3
          ch: 64
          ch_mult: [1, 2, 4, 4]    # num_down = len(ch_mult) - 1  → 3 downsamples ⇒ 8× downsampling
          num_res_blocks: 2
          attn_resolutions: []      # no self-attn in the VAE
          dropout: 0.0
        embed_dim: 16

    patch_critic:
      path: 'rhadversarial.nn.model_zoo.patchgan.patchgan_discriminator'
      name: 'Discriminator'
      params:
        in_channels: 3
        spectral_norm_enabled: True

    image_critic:
        path: rhadversarial.nn.model_gpt.resgan.resgan_discriminator
        name: Discriminator
        params: 
          input_scale: 128
          base_channels: 32
          output_scale: 4
          attn_at: [8]

  losses:
    pixel_loss:
      path: 'torch.nn'
      name: 'MSELoss'
      params: {}
      weight: 0.5

    lpf_loss:
      path: 'rhcompression.nn.losses.lowpass_freq_loss'
      name: 'CutoffControlled'
      params: {}
      weight: 0.5

    critic:
      path: 'rhino.nn.losses.gan_losses'
      name: 'HingeGANLoss'
      params: {}
      wrapper: # This wrapper is more complex, as it needs parameters
        mpath: 'rhcore.nn.losses.wrappers.list_of_keys.ListOfKeys'
        params:
          keys: ["real", "fake"] # targets is x, preds is params
      weight_scheduler:
        path: 'rhcore.loss_weight_schedulers.fadein'
        name: 'SigmoidFadeIn'
        params:
          max_weight: 0.002
          start_step: 0
          duration: 20000
          step_size: 5
          eps: 0.000000001

optimization:
  target_loss_key: 'total_loss'
  net:
    path: 'torch.optim'
    name: 'Adam'
    params:
      lr: 0.0001
      betas: [0.0, 0.99]
      weight_decay: 0.0
    lr_scheduler: {}

  critic:
    path: 'torch.optim'
    name: 'Adam'
    bind_to: ['patch_critic', 'image_critic']
    params:
      lr: 0.0002
      betas: [0.0, 0.99]
      weight_decay: 0.0
    lr_scheduler: {}

  use_ema:
    ema_decay: 0.999
    ema_start: $ema_start

trainer:
  devices: $devices
  max_epochs: $max_epochs
  val_check_epochs: 1
  state_save_epochs: 1
  fup_by_key: # Setup find unused parameters, this is particular useful for DDP. 
    net: True

_vars:
  devices: [2, 3, 4, 5]
  max_epochs: 500

  train_batch_size: 64
  valid_batch_size: 16
  num_workers: 16

  # point these to your datasets (normalized to [-1,1] in datamodule)
  train_folder_paths:
    gt: data/ffhq/ffhq_imgs/ffhq_128
  valid_folder_paths:
    gt: data/celeba/subsets/celeba_128

  experiment_name: msvae_ffhq_128_high_kl
  ema_start: 1000
